# -*- coding: utf-8 -*-
"""xor_knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OEAcOqv_6vTmiVjKWTI20GNrbGtWso3b
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

rng = np.random.RandomState(0)

# x1 = np.concatenate([np.random.uniform(50, 100, 10), np.random.uniform(-100, -50, 10)])
# y1 =np.concatenate([np.random.uniform(-100, -50, 10), np.random.uniform(50, 100, 10)])

# x2 = np.concatenate([np.random.uniform(50, 100, 10), np.random.uniform(-100, -50, 10)])
# y2 =np.concatenate([np.random.uniform(50, 100, 10), np.random.uniform(-100, -50, 10)])

# xx = np.concatenate((x1, x2))
# yy = np.concatenate((y1, y2))

# X = np.array(list(zip(xx,yy)))
# y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

X = rng.randn(200, 2)
y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

print(X)
print(y)

# plt.scatter(x1, y1, c='red')
# plt.scatter(x2, y2, c='blue')
# plt.show()

dic = {0 : 'red', 1 : 'blue'}

def Euclidean(pA, pB):
  return np.sum((pA-pB)**2)**0.5

def knn(X, y, new_point, scal):
  train = X.shape[0]
  distances = []

  for i in range(train):
    dis = Euclidean(new_point, X[i])
    distances.append((dis, y[i]))
  
  distances = sorted(distances)
  distances = distances[:scal]

  distances = np.array(distances)
  labels = distances[:,1]

  unique_labels, counts = np.unique(labels, return_counts=True)
  pred = unique_labels[counts.argmax()]

  return dic[int(pred)]

def main():
  new_point = np.array([1, -2])
  print(knn(X, y, new_point, 5))
  plt.scatter(X[:,0], X[:,1], c = colors[(y.astype(int))])
  plt.scatter(1, -2, c = 'black')


main()